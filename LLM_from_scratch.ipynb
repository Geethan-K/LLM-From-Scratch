{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35659bff-5c88-426a-9f74-34fd800d6edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of. charachters 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\" , \"r\" , encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total no. of. charachters\" , len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4ab98c70-4df6-4f19-b229-1e7406a1963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', ' ', '!', ' ', ',', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello world ! , This is a test\"\n",
    "result = re.split(r'(\\s)' , text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eba21807-1a6e-4a60-99ea-5ecca120e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', ' ', '', '!', '', ' ', '', ',', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.;:?_!\"()\\']|--|\\s)' , text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f2b270b4-911f-4628-a674-fb4d4cb6f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', ',', 'This', 'is', 'a', 'test']\n"
     ]
    }
   ],
   "source": [
    "## removing the white spaces\n",
    "result = [item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6c3ece04-cde3-4b62-b4b4-06cd1bb55494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)' , raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04700b6f-ff08-4234-a252-44b0dbfe7e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51146b11-ed38-4e16-a6a4-2843e41facfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "### sort all words in alfabetical order \n",
    "## set keyword is used to take only unique words (non-repeating)\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "37206536-648e-4d24-ab1b-7e7d3237e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30a145d8-fb5b-401e-86dc-85b894a21cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n"
     ]
    }
   ],
   "source": [
    "## Let's print some of them to understand \n",
    "\n",
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if(i>=20):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c321fec3-8ab9-4262-ad07-8c7b06490492",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE ABOVE PROCESS IS ENCODER \n",
    "### reversal of this process is called DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0529185d-21f6-4edf-a266-8edddd0ba9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab  ##mentioned above list\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)' , text)\n",
    "        preprocessed = [ item.strip() for item in preprocessed if item.strip() ]\n",
    "        ids = [ self.str_to_int[s] for s in preprocessed ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        ## Replace spaces before special charachters\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])' , r'\\1' , text)\n",
    "        return text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "33a4cb0b-f006-4da6-8bc4-6ee3a1dfcf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's test it\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted you know,\"\n",
    "        Mrs. Gisburn said with pardonable pride.\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71d04fee-b2b1-48ae-80ce-d9c049d70507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "### Test for encode function whether it convert these text into ids and returning it \n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7c9a0f67-0d96-4b38-bb4c-175e19800b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db9a17bf-383d-442a-a591-91a950a964be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### What if we add text that is not in book ?\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"Let's have tea ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2dccfff8-b578-4e75-b117-4308ee918a09",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Let'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(ids)\n",
      "Cell \u001b[1;32mIn[73], line 9\u001b[0m, in \u001b[0;36mSimpleTokenizerV1.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m([,.;:?_!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]|--|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms)\u001b[39m\u001b[38;5;124m'\u001b[39m , text)\n\u001b[0;32m      8\u001b[0m preprocessed \u001b[38;5;241m=\u001b[39m [ item\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip() ]\n\u001b[1;32m----> 9\u001b[0m ids \u001b[38;5;241m=\u001b[39m [ \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed ]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Let'"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7917dcde-dce5-4f33-9cb2-26d51784b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### See the above wont't work as these keywords are not in book\n",
    "### So , Let's add some more tokens like unknown , end of text .\n",
    "\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e60db35-87fa-4cb4-97c6-44eed8865c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ebae8c9-d8c9-402b-b7ff-0dc86636085d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "## Let's print last 5 words in vocab\n",
    "for i,item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cdef7503-73c8-4b01-894a-fbf36cc6abf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int = vocab  ##mentioned above list\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self,text):\n",
    "        preprocessed = re.split(r'([,.;:?_!\"()\\']|--|\\s)' , text)\n",
    "        preprocessed = [ item.strip() for item in preprocessed if item.strip() ]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [ self.str_to_int[s] for s in preprocessed ]\n",
    "        return ids\n",
    "\n",
    "    def decode(self,ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        ## Replace spaces before special charachters\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])' , r'\\1' , text)\n",
    "        return text        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "66f32cab-62f1-44d4-b18c-8585d32dba43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea?<|endoftext|>In the sunlight terraces of the palace. \n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlight terraces of the palace. \"\n",
    "\n",
    "text = \"<|endoftext|>\".join((text1,text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "857b0a30-657b-4469-bb81-d106d453c90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1131, 988, 1131, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "adb0a365-50d1-4aab-92b7-1574a0808ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|unk|> the <|unk|> terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72300348-c3e5-4ffe-9220-e3b8123fa9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version\" , importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61187608-f541-41eb-abc5-134fa3b07236",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the Byte pair tokenizer from GPT\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "32f3c6db-b931-4f15-81c6-9bab2bd60acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 5372, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownplace.\"   \n",
    "       )\n",
    "\n",
    "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e907f168-2aab-4935-8f42-4f577f0e32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownplace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd1f9d3c-2b74-47dd-8aa1-bd1905da728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\" , \"r\" , encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3ce636a2-c35c-499f-9a51-7fda62222eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the first 50 tokens for demonstration \n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0100ed38-0dca-4f9e-8a2a-bbc0009108b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:[290, 4920, 2241, 287]\n",
      "y:[4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 ## Length of the input\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x:{x}\")\n",
    "print(f\"y:{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5228bb71-017c-4062-b543-9989c75a6f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] -----> 4920\n",
      "[290, 4920] -----> 2241\n",
      "[290, 4920, 2241] -----> 287\n",
      "[290, 4920, 2241, 287] -----> 257\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1): ## Run from 1 to 5\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context , \"----->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7925e60e-8d32-4032-ba3e-ca928ce0ec8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ----->  established\n",
      " and established ----->  himself\n",
      " and established himself ----->  in\n",
      " and established himself in ----->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1): ## Run from 1 to 5\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context) , \"----->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "182f8c57-c22a-4b27-9f9a-d85a4bc936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above is a simple demonstration . For larger data processing we use data loaders\n",
    "### We will use pytorch's built in Dataset and Dataloader classes\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        #Use the sliding window approach . \n",
    "        for i in range(0,len(token_ids) - max_length , stride):\n",
    "            input_chunk = token_ids[i: i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk)) ## 1'st row i.e X value [from 0 --> 4]\n",
    "            self.target_ids.append(torch.tensor(target_chunk))  ## 2'nd row i.e Y value [from 1 --> 5]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self,idx):  ## if the idx--> index is 50 (50th row)\n",
    "        return self.input_ids[idx] , self.target_ids[idx]    ## it will return 50th row of input tensor and 50th row of output tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb246f55-cf96-4bbe-94ae-9d60a6ac1f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt , batch_size=4, max_length=256 , stride=128 , shuffle=True , drop_last=True , num_workers=0):\n",
    "    ## txt --> verdict.txt\n",
    "    ## batch_size --> run 4 process parallelly in cpu\n",
    "    ## max_length --> context size (previously we used 4 , GPT uses 256)\n",
    "    ## stride --> How much (words) we need to skip before the next batch\n",
    "    ## drop_last --> drop the last row , if row size (last batch) is shorter than specified batch size , to prevent loss spikes during training \n",
    "    \n",
    "    ## initialize the tokenizer --> we are using BytePair Tokenizer from GPT\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    ## create Dataloader\n",
    "    dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "\n",
    "    #create Dataloader\n",
    "    ## checks for the __get_item method in GPTDatasetV1 and returns the item\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a479f2-e992-4ab3-93d0-ba1115a1b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's test the dataloader with batch size of 1 for an LLM with context size of 4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2156fc34-126e-4730-b247-6af1212b22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\" , \"r\" , encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b45383bc-eca1-4313-bc4d-6de99052c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 2.8.0+cpu\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"pytorch version:\" , torch.__version__)\n",
    "dataloader = create_dataloader_v1(raw_text , batch_size=1 , max_length=4 , stride=1 , shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e3857f5f-872a-4097-8925-6443541eb801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "65825512-fa98-45e5-bdb7-5c3904edc2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs : \n",
      " : tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      " Targets : \n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text , batch_size=8 , max_length=4 , stride=4 , shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)\n",
    "print(\"Inputs : \\n :\", inputs)\n",
    "print(\"\\n Targets : \\n\" , targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccd658-1406-4d8a-8f6d-5234f265ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converts words to 300 dim vectors\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e92544-e92a-4993-ab93-5469ad21ff7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_vectors[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "word_vectors = model\n",
    "\n",
    "print(word_vectors[\"computer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10702cb0-2b86-4320-8b80-df0e3def993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors[\"cat\"].shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd2c60-5680-4618-9453-954d5efcbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.most_similar(positive=['king','woman'] , negative=[man] , topn=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb4e18f-3273-4540-b50f-f067682e4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's check the familarity b/w a few pair of words\n",
    "\n",
    "print(word_vectors.similarity('woman' , 'man'))\n",
    "print(word_vectors.similarity('king' , 'queen'))\n",
    "print(word_vectors.similarity('uncle' , 'aunt'))\n",
    "print(word_vectors.similarity('boy' , 'girl'))\n",
    "print(word_vectors.similarity('paper' , 'water'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123402e2-719b-4678-9987-5d849095e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_vectors.most_similar(\"tower\",topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff163f15-397d-4ba7-aeb4-38585e99a354",
   "metadata": {},
   "outputs": [],
   "source": [
    "## example of Absolute positional encoding\n",
    "\n",
    "max_length = 4\n",
    "data_loader = create_dataloader_v1(\n",
    "    raw_text,batch_size=8,max_length=max_length,stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "inputs,targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bafc077-4289-4b32-87cc-5c16b363414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID's:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      " Inputs shape : \n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token ID's:\\n\" , inputs )\n",
    "print(\"\\n Inputs shape : \\n\" , inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01930cb3-31bc-42ea-b5ec-73e5a6f33e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257 ## (rows)\n",
    "output_dim = 256 ## (cols --> features --> dimensions )\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bd863ac-fcb7-48e3-bbde-66a2fe11ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "## For each token id , we need to create 256 dimensional vector~(tensor shape)\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dda8ecaf-9acd-40c6-9d19-79db396ca748",
   "metadata": {},
   "outputs": [],
   "source": [
    "## But for positional embedding layer , we need only 4 as the batch size is only 4\n",
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d168d87-a763-4097-a2d3-12ffa980cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "## We need only 4 positions [0,1,2,3] for a token ID as it is same for each row/ID\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269fac43-f2aa-4ae2-a646-b638a9a1d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing a simplified attention mechanism\n",
    "\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "        [0.43, 0.15 , 0.89],  # Your\n",
    "        [0.55, 0.87 , 0.66],  # journey\n",
    "        [0.57 , 0.85 , 0.64],  # starts\n",
    "        [0.22 , 0.58 , 0.33],  # with\n",
    "        [0.77 , 0.25 , 0.10],  # one\n",
    "        [0.05 , 0.80 , 0.55]   # step\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51d4ccd-705f-4f64-a256-b83b1adba8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Corresponding words\n",
    "words = [\"Your\" , \"journey\" , \"starts\" , \"with\" , \"one\" , \"step\"]\n",
    "\n",
    "#Extract X , Y , Z coordinates\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111 , projection='3d')\n",
    "\n",
    "## Plot each point and annotate with corresponding word\n",
    "\n",
    "for x , y , z , word in zip( x_coords , y_coords , z_coords , words):\n",
    "    ax.scatter(x,y,z)\n",
    "    ax.text(x,y,z,word,fontsize=10)\n",
    "\n",
    "## Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_xlabel('Y')\n",
    "ax.set_xlabel('Z')\n",
    "\n",
    "plt.title('3D plot of word embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc237e5-9a5b-46ad-9c16-6c04dce46e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465053b7-8f2a-423d-a4a9-ddd828dece90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To calculate the attention score of query vector , we need to calculate dot product with each input vectors\n",
    "\n",
    "query = inputs[1]  # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0]) # First initialize the attention score as a empty tensor\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3873818-fc5c-4f94-b8b0-20bc4c513841",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a06d9-64d6-462a-9738-661cd85135b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### In practice, it's more common and advisable to use the softmax function for normalization.\n",
    "### This approach is better at managing extreme values and offers more favorable gradient properties during training. \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd232e17-a00c-4d30-a227-062749b41a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c08f9df-1d6e-46df-9595-6a954984967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2e494-a939-405d-887e-2d4b4a3145ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55], # step     (x^6)\n",
    "   [0.4419, 0.6515, 0.5683]]\n",
    ")\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step', 'journey-context']\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each point and annotate with corresponding word\n",
    "for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings')\n",
    "plt.show()\n",
    "\n",
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'r']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec3414e-e78f-4766-9e10-7fb69f85a597",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Now, we can extend this computation to calculate attention weights and context vectors for all inputs.\n",
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba968468-cef1-4d5b-97c0-f8108fac2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### When computing the preceding attention score tensor, we used for-loops in Python.                                                          \n",
    "## However, for-loops are generally slow, and we can achieve the same results using matrix multiplication:\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd88291-7082-46f5-b40b-d0a92478bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now normalize each row so that the values in each row sum to 1:\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc47921-58bf-4ee8-ab78-de3acbc17ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the context of using PyTorch, the dim parameter in functions like torch.softmax specifies the dimension of the input tensor along which the function will be computed. \n",
    "## By setting dim=-1, we are instructing the softmax function to apply the normalization along the last dimension of the attn_scores tensor. \n",
    "\n",
    "## If attn_scores is a 2D tensor (for example, with a shape of [rows, columns]), dim=-1 will normalize across the columns so that the values in\n",
    " ## each row (summing over the column dimension) sum up to 1.\n",
    "\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dce9852-a219-4f09-b1ea-0954c3e356e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the third and last step, we now use these attention weights to compute all context vectors via matrix multiplication:\n",
    "\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fcb54d-29b5-4bcf-ac3b-e224fceb33a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can double-check that the code is correct by comparing the 2nd row with the context vector z(2) calculated previously\n",
    "\n",
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c71f06-553c-4518-8516-2842f8e3f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPLEMENTING SELF ATTENTION WITH TRAINABLE WEIGHTS\n",
    "\n",
    "x_2 = inputs[1] #A\n",
    "d_in = inputs.shape[1] #B\n",
    "d_out = 2 #C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9d044-3e8a-4ad6-93c7-61c8889ac39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we initialize the three weight matrices Wq, Wk and Wv\n",
    "\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe15331-e172-4a11-90da-cdf1e4d5be7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next, we compute the query, key, and value vectors as shown earlier\n",
    "\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079393-c67a-4823-8da1-59b5bc1c1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As we can see based on the output for the query, this results in a 2-dimensional vector. \n",
    "## This is because: we set the number of columns of the corresponding weight matrix, via d_out, to 2:\n",
    "## Even though our temporary goal is to only compute the one context vector z(2),  we still require the key and value vectors for all input elements. \n",
    "## This is because they are involved in computing the attention weights with respect to the query q(2)\n",
    "\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea8f13-e85b-4503-940a-fa5043fed70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, let's compute the attention score ω22\n",
    "\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66850dd1-7d64-4f51-8c15-3d4f986773c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Again, we can generalize this computation to all attention scores via matrix multiplication:\n",
    "\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a482d0f-6898-4397-91a0-162ce6842353",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We compute the attention weights by scaling the attention scores and using the softmax function we used earlier. \n",
    "## The difference to earlier is that we now scale the attention scores by dividing them by the square root of the\n",
    " #embedding dimension of the keys. \n",
    "\n",
    "##Note that taking the square root is mathematically the same as exponentiating by 0.5:\n",
    "\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad2291-3a1e-4b92-92d7-0a8beeb2a673",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now compute the context vector as a weighted sum over the value vectors. \n",
    "\n",
    "## Here, the attention weights serve as a weighting factor that weighs the respective importance of each value vector. \n",
    "\n",
    "## We can use matrix multiplication to obtain the output in one step\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea711180-30a8-4961-b747-b9b0723a2f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPLEMENTING A COMPACT SELF ATTENTION PYTHON CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c586c3f9-143a-4e3d-9bf1-a09addb37c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f375317-abaf-4557-bcaf-d905afdcceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally, we create a context vector by weighting the values with these normalized attention scores.\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff74b6d-eeb7-40b2-9449-2b642fa4f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Since inputs contains six embedding vectors, we get a matrix storing the six context vectors, as shown in the above result. \n",
    "## As a quick check, notice how the second row ([0.3061, 0.8210]) matches the contents of context_vec_2 in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedd0cfe-488c-4e8c-b0a8-04667132a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can improve the SelfAttention_v1 implementation further by utilizing PyTorch's nn.Linear layers, which effectively perform matrix \n",
    "# multiplication when the bias units are disabled. \n",
    "\n",
    "## Additionally, a significant advantage of using nn.Linear instead of manually\n",
    " ## implementing nn.Parameter(torch.rand(...)) is that nn.Linear has an optimized weight\n",
    " ## initialization scheme, contributing to more stable and effective model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7b5d6-4551-4c06-9eff-b20bb0470e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e50b0-7354-481b-a3d4-3bf8d5ec0296",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use the SelfAttention_v2 similar to SelfAttention_v1:\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
